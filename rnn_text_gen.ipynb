{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project goal is to create an RNN that operates at the character level to generate text. This means, given a sequence of characters, the RNN will generate the next character in the sequence. If the model is trained well, this can be done repeatedly to generate text that resembles the training corpus. I'll try to make everything as generic as possible so any source text can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600893\n"
     ]
    }
   ],
   "source": [
    "# Import the text file as one big string\n",
    "path = \"../data/\"\n",
    "filename = \"nietzsche.txt\"\n",
    "text = open(f'{path}{filename}').read()\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Æ', 'ä', 'æ', 'é', 'ë']\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The vocabulary is all the unique symbols used in the text. This is the real benefit of working with a character-\n",
    "level RNN. The vocabulary is tiny, and you don't need to deal with unkown words.\n",
    "\"\"\"\n",
    "chars = sorted(set(text))\n",
    "print(type(chars))\n",
    "print(chars)\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries from character->index and index->character\n",
    "c_to_idx = {c:i for i, c in enumerate(chars)}\n",
    "idx_to_c = {i:c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39, 41, 28, 29, 24, 26, 28, 0, 0, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert whole text to indices. Want each character to be represented by its index in the vocabulary. This is how\n",
    "# we'll feed it to the RNN.\n",
    "text_idx = [c_to_idx[c] for c in text]\n",
    "text_len = len(text_idx)\n",
    "text_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth is a woman--what then? Is there not ground\n",
      "for suspecting that all ph\n",
      "-----------\n",
      "Truth is a woman--what then? Is there not ground\n",
      "for suspecting that all ph\n"
     ]
    }
   ],
   "source": [
    "# Check it works to convert back: Join up the indices\n",
    "print(text[25:100])\n",
    "print(\"-----------\")\n",
    "print(''.join([idx_to_c[i] for i in text_idx[25:100]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence of characters passed to RNN at a time. This dictates the length of the unrolled model (# timesteps)\n",
    "# Batch size affects splitting of raw data as well as model architecture.\n",
    "seq_len = 8\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want a non-verlapping set of inputs and outputs. Each X should be equal to the sequence length. So should Y,\n",
    "# but should be shifted by 1. Want to shift X by the sequence length each step.\n",
    "# Note that we don't go right to the end so that there's room for Y and for a whole final sequence.\n",
    "idx_in_data = [text_idx[idx:idx+seq_len] for idx in range(0, text_len-1-seq_len, seq_len)]\n",
    "\n",
    "# idx_in_data = [[text_idx[idx] for idx in range(text_pt, text_pt+seq_len)] \\\n",
    "#                for text_pt in range(0, text_len - 1 - seq_len, seq_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75111, 8)\n",
      "[[39 41 28 29 24 26 28  0]\n",
      " [ 0  0 42 44 39 39 38 42]\n",
      " [32 37 30  1 72 60 53 72]]\n"
     ]
    }
   ],
   "source": [
    "# Convert these inputs into a numpy array and provide some info. Note that the dimensions are the total number of\n",
    "# sequences in the corpus and the sequence length\n",
    "inp = np.array(idx_in_data)\n",
    "print(inp.shape)\n",
    "print(inp[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same thing for Y\n",
    "idx_out_data = [text_idx[idx:idx+seq_len] for idx in range(1, text_len-seq_len, seq_len)]\n",
    "\n",
    "# idx_out_data = [[text_idx[idx] for idx in range(text_pt, text_pt+seq_len)] \\\n",
    "#                 for text_pt in range(1, text_len - seq_len, seq_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75111, 8)\n",
      "[[41 28 29 24 26 28  0  0]\n",
      " [ 0 42 44 39 39 38 42 32]\n",
      " [37 30  1 72 60 53 72  1]]\n"
     ]
    }
   ],
   "source": [
    "# Confirm that the target array is the input array shifted by 1. We'll be predicting the next character in the\n",
    "# sequence.\n",
    "outp = np.array(idx_out_data)\n",
    "print(outp.shape)\n",
    "print(outp[:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Split up the input and target data into training and test sets.\n",
    "Return 4 numpy arrays - training input, training targets, test input, and test targets\n",
    "'''\n",
    "def train_test_split(inp_data, out_data, train_fraction):\n",
    "    trn_idx = np.random.rand(len(inp_data)) < train_fraction\n",
    "    \n",
    "    inp_trn = inp_data[trn_idx]\n",
    "    inp_test = inp_data[~trn_idx]\n",
    "    \n",
    "    outp_trn = out_data[trn_idx]\n",
    "    outp_test = out_data[~trn_idx]\n",
    "    return inp_trn, outp_trn, inp_test, outp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 90% training, 10% test. This ratio could change with bigger corpus\n",
    "x_trn, y_trn, x_val, y_val = train_test_split(inp, outp, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PyTorch Dataset class for character level text generation.\n",
    "X and Y have widths equal to the sequence length.\n",
    "'''\n",
    "class CharSeqDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X);\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.X[idx];\n",
    "        label = self.Y[idx];\n",
    "        \n",
    "        return (item, label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation datasets\n",
    "train_ds = CharSeqDataset(x_trn, y_trn)\n",
    "val_ds = CharSeqDataset(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn these into PyTorch dataloaders with batch size = batch_size.\n",
    "# This will take care of the shuffling and batching,.\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True);\n",
    "val_dl = DataLoader(dataset=val_ds, batch_size=batch_size, shuffle=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple experiments with the data loaders.  \n",
    "1. The X and Y values are paired. Show that shuffling keeps the lined up.\n",
    "2. You get a different order whenever you iterate over a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_iter = iter(train_dl)\n",
    "x_exp, y_exp = next(exp_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[ 1, 53, 66, 56,  1, 66, 67, 65],\n",
      "        [67, 66, 57,  1, 71, 67, 73, 64]])\n",
      "*****\n",
      "torch.Size([512, 8])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[53, 66, 56,  1, 66, 67, 65, 53],\n",
      "        [66, 57,  1, 71, 67, 73, 64,  9]])\n"
     ]
    }
   ],
   "source": [
    "# Exp 1.\n",
    "print(x_exp.shape) # batch size by sequence length\n",
    "print(type(x_exp))\n",
    "print(x_exp[:2, :])\n",
    "print(\"*****\")\n",
    "print(y_exp.shape)\n",
    "print(type(y_exp))\n",
    "print(y_exp[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[73, 68, 57, 70, 58, 61, 55, 61],\n",
      "        [53, 75, 53, 77, 20,  1, 72, 60]])\n",
      "*****\n",
      "torch.Size([512, 8])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[68, 57, 70, 58, 61, 55, 61, 53],\n",
      "        [75, 53, 77, 20,  1, 72, 60, 57]])\n"
     ]
    }
   ],
   "source": [
    "# Exp 2.\n",
    "exp_iter2 = iter(train_dl)\n",
    "x_exp2, y_exp2 = next(exp_iter2)\n",
    "\n",
    "print(x_exp2.shape) # batch size by sequence length\n",
    "print(type(x_exp2))\n",
    "print(x_exp2[:2, :])\n",
    "print(\"*****\")\n",
    "print(y_exp2.shape)\n",
    "print(type(y_exp2))\n",
    "print(y_exp2[:2, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character level RNN model class, using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension for character's learned embeddings. Number of hidden units in the RNN\n",
    "emb_dim = 42\n",
    "n_hidden = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pytorch model.\n",
    "One sequence step involves embedding layer->RNN->fully connected layer->softmax over vocabulary\n",
    "A couple tricky points:\n",
    "-Want to keep the hidden activation values after a forward pass. So I have to detach h after a \n",
    "forward pass so BPTT doesn't have to go through all the steps back to the very beginning of the corpus.\n",
    "-Output predictions are rank 3 tensor of batch_size x seq_len x vocab length (it's a prediction over the vocab\n",
    "for each char in the sequence and for each sequence in the minibatch). Softmax only accepts rank 2, so need to\n",
    "reshape this into a (batch_size * seq_len) x vocab_length tensor.\n",
    "'''\n",
    "class CharRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, bs):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, emb_dim) # Going from vocab size down to embedding size\n",
    "        # Automatically runs for N sequence steps, which is known from input data size\n",
    "        self.rnn = nn.RNN(emb_dim, n_hidden) # embedding size to number of hidden units\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.h = self.init_h(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs.shape[0]\n",
    "        if self.h.shape[1] != bs:\n",
    "            self.h = self.init_h(bs)\n",
    "        inp = self.e(cs)\n",
    "        inp = torch.transpose(inp, 0, 1)\n",
    "        outp, h = self.rnn(inp, self.h)\n",
    "        self.h = Variable(h.data) # Save hidden values for next forward pass. Remove from BPTT by rewrapping in Var\n",
    "        outp = F.log_softmax(self.l_out(outp), dim=-1)\n",
    "        outp = torch.transpose(outp, 0, 1)\n",
    "        return outp.contiguous().view(-1, vocab_size) #This is tricky! Write myself a note it\n",
    "    \n",
    "    def init_h(self, bs):\n",
    "        return Variable(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function does 1 epoch (pass through the data)\n",
    "def train(model, opt, crit, train_loader):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        opt.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        targets = targets.view(-1)\n",
    "        loss = crit(outputs, targets)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.data);\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function calculates average loss over all the test data.\n",
    "def test(model, test_loader, crit):\n",
    "    # Put model in evaluation mode. Read up on what it does\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            targets = targets.view(-1)\n",
    "#             l = F.nll_loss(outputs, targets, reduction='sum').item() / len(targets)# sum up batch loss\n",
    "            l = crit(outputs, targets)\n",
    "            test_loss += l.item()\n",
    "            pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability (char index)\n",
    "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRnn(vocab_size, emb_dim, batch_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 4, Training Loss: 2.5601, Validation Loss: 2.4270\n",
      "Epoch: 2 / 4, Training Loss: 2.3219, Validation Loss: 2.2324\n",
      "Epoch: 3 / 4, Training Loss: 2.1566, Validation Loss: 2.1364\n",
      "Epoch: 4 / 4, Training Loss: 2.0871, Validation Loss: 2.0669\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "for ep in range(epochs):\n",
    "    tr_loss = train(model, optimizer, criterion, train_dl)\n",
    "    test_loss = test(model, val_dl, criterion)\n",
    "    print(f'Epoch: {ep+1} / {epochs}, Training Loss: {tr_loss[-1]:.4f}, Validation Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing - look at this quantitatively and see how well it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Given an input and a trained model, do a forward pass and predict the next character in the input sequence.\n",
    "Return this character as its integer index in the vocabulary.\n",
    "'''\n",
    "def next_letter(my_model, inp):\n",
    "\n",
    "    inp = torch.tensor([inp])\n",
    "    model_out = my_model(inp)\n",
    "    # Grab the last letter from the model output\n",
    "    # And sample from the vocabulary based on the weighted probability for character in the vocab.\n",
    "    # This makes this result non-deterministic, there can be variance between the next letter in the sequence\n",
    "    # depending on the sampling. Especially if multiple character get assigned similar probabilities.\n",
    "    next_idx = torch.multinomial(model_out[-1].exp(), 1).item()\n",
    "    \n",
    "    # return the next character index in the sequence\n",
    "    return next_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "mytext = \"thos\"\n",
    "mytext = [c_to_idx[i] for i in mytext]\n",
    "nl = next_letter(model, mytext)\n",
    "print(nl, idx_to_c[nl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Keep generating the next character in the sequence. Repeatedly move the sampling window to include the latest\n",
    "prediction and predict the next letter. Goes for num_chars repetitions.\n",
    "'''\n",
    "def gen_text(my_model, inp, num_chars):\n",
    "    text = inp\n",
    "    inp = [c_to_idx[i] for i in inp]\n",
    "    for c in range(num_chars):\n",
    "        l = next_letter(my_model, inp)\n",
    "        text += idx_to_c[l]\n",
    "        inp = inp[1:]+[l]\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 57, 64, 64, 67]\n",
      "Hellood, and the eaZme manurigatian, that mign or of wat work le conmpteng cighthis\n",
      "prearis spodivy, a detion yspgecate sthe  Lne that eactimpion they\n",
      "Gixl agold, and ithenyen ne thesest? If for trumal, ny\n",
      "infureing in they,\n",
      "simpired that aspence and ehation fecatint mord mustinct from it unether to the\n",
      "reea4isself, is acthis of trey are\n",
      "insclof watsiat apal anyithit\n",
      " tromatind, and Jenation esell as t\n"
     ]
    }
   ],
   "source": [
    "gen_text(model, \"Hello\", 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like it's starting to work alright, especially since it hasn't trained for too long. I found that benefits of continued training started to level off after ~30 epochs or so.\n",
    "\n",
    "I learned something important here though. When I split up the corpus into sequences of length 8 (sequence length / bptt length), characters 1 - 8 are the first training example in batch 1, 9 - 16 are the second etc.\n",
    "What that means is that the hidden states after the forward pass are meaningless for the next batch. There's no information gained about the previous sequence to help you out with the current sequence! \n",
    "\n",
    "Here's a different idea -> What if characters 1 - 8 make up the first training example of the first batch, then characters 9 - 16 make up the first training example of the second batch. That way (since we're saving activation values) when character 9 gets passed in as the first step to the RNN, the activations correspond to what came out after character 8, which was the last character of example 1 in the previous minibatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data up into \"vertical stacks\" as explained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 600893\n",
      "Batch size: 512\n",
      "Sequence length / bptt length: 8\n"
     ]
    }
   ],
   "source": [
    "print(f'Corpus length: {len(text)}')\n",
    "print(f'Batch size: {batch_size}')\n",
    "print(f'Sequence length / bptt length: {seq_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to split the corpus into a number of chunks equal to the number of mini batches (512) because each chunk will represent a row example in successive minibatches. Also, the sequences need to still be seq_len long. So it's easiest to figure out how long (number of chars) a block can be if we need to get 512 into the corpus, then round that length to something evenly divisible by the sequene length. We lose a little bit of potential information, but it MAY be easier than having the final minibatch have a shorter sequence. Another option would be to zero-pad that last sequence, but I'm not going to worry about that since there's a lot of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-Pass in text, batch size, and sequence length to get back numpy array where consecutive text is lined up\n",
    "across minibatches.\n",
    "-Remember, in a list comprehension, the second for executes fully (it's the nested one). Each pass through the text, \n",
    "we grab a sequence lengthed bit of text from each \"mini-batch block\". Then next pass the index is shifted over.\n",
    "The idea is that you build an array where mini_batch example i makes continuous text across the mini batches,\n",
    "rather than within a minibatch.\n",
    "'''\n",
    "def vertical_chunk(text, bs, sl):\n",
    "    s_per_block = len(text) // sl // bs\n",
    "    c_per_block = s_per_block * sl\n",
    "    tl = c_per_block * bs\n",
    "    \n",
    "    r = [text[b+i : b+i+sl] for i in range(0,c_per_block,sl) for b in range(0,tl,c_per_block)]\n",
    "    return np.array(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_inp = vertical_chunk(text_idx, batch_size, seq_len)\n",
    "stacked_outp = vertical_chunk(text_idx[1:], batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74752, 8)\n",
      "(74752, 8)\n"
     ]
    }
   ],
   "source": [
    "print(stacked_inp.shape)\n",
    "print(stacked_outp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 41, 28, 29, 24, 26, 28, 0, 0, 0, 42, 44, 39, 39, 38, 42]\n",
      "********\n",
      "[39 41 28 29 24 26 28  0]\n",
      "[41 28 29 24 26 28  0  0]\n",
      "********\n",
      "[ 0  0 42 44 39 39 38 42]\n",
      "[ 0 42 44 39 39 38 42 32]\n"
     ]
    }
   ],
   "source": [
    "# Show that continuous text is split over minibatch indices\n",
    "print(text_idx[:16])\n",
    "print(\"********\")\n",
    "print(stacked_inp[0])\n",
    "print(stacked_outp[0])\n",
    "print(\"********\")\n",
    "print(stacked_inp[512])\n",
    "print(stacked_outp[512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to make a dataloader. But we don't want to shuffle the data, because the continuity is important for the activations. So just split up the data into test and train by index, then make a dataloader without shuffle on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't want to randomly split. Just take the first half\n",
    "# st_x_trn, st_y_trn, st_x_val, st_y_val = train_test_split(stacked_inp, stacked_outp, 0.9)\n",
    "def data_split_nonrandom(in_data, out_data, train_frac):\n",
    "    portion = int(len(in_data) * train_frac)\n",
    "    return in_data[:portion], out_data[:portion], in_data[portion:], out_data[portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_x_trn, st_y_trn, st_x_val, st_y_val = data_split_nonrandom(stacked_inp, stacked_outp, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation datasets\n",
    "st_train_ds = CharSeqDataset(st_x_trn, st_y_trn)\n",
    "st_val_ds = CharSeqDataset(st_x_val, st_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unshuffled dataloaders\n",
    "# Is there a way to shuffle cross batch? Does this even have a point?\n",
    "st_train_dl = DataLoader(dataset=st_train_ds, batch_size=batch_size, shuffle=False);\n",
    "st_val_dl = DataLoader(dataset=st_val_ds, batch_size=batch_size, shuffle=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "tensor([39, 41, 28, 29, 24, 26, 28,  0])\n",
      "tensor([41, 28, 29, 24, 26, 28,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Test that we're not shuffling\n",
    "test_iter = iter(st_train_dl)\n",
    "x_test, y_test = next(test_iter)\n",
    "print(x_test.shape)\n",
    "print(x_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "tensor([ 0,  0, 42, 44, 39, 39, 38, 42])\n",
      "tensor([ 0, 42, 44, 39, 39, 38, 42, 32])\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = next(test_iter)\n",
    "print(x_test.shape)\n",
    "print(x_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "tensor([32, 37, 30,  1, 72, 60, 53, 72])\n",
      "tensor([37, 30,  1, 72, 60, 53, 72,  1])\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = next(test_iter)\n",
    "print(x_test.shape)\n",
    "print(x_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 37, 30, 1, 72, 60, 53, 72]\n"
     ]
    }
   ],
   "source": [
    "print(text_idx[16:24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with vertically stacked data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data all looks great now. Try training the existing model and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_model = CharRnn(vocab_size, emb_dim, batch_size)\n",
    "st_optimizer = torch.optim.Adam(st_model.parameters(), lr=1e-3)\n",
    "st_criterion = nn.CrossEntropyLoss();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 30, Training Loss: 2.4439, Validation Loss: 2.3349\n",
      "Epoch: 2 / 30, Training Loss: 2.2350, Validation Loss: 2.0820\n",
      "Epoch: 3 / 30, Training Loss: 2.1160, Validation Loss: 1.9402\n",
      "Epoch: 4 / 30, Training Loss: 2.0412, Validation Loss: 1.8488\n",
      "Epoch: 5 / 30, Training Loss: 1.9929, Validation Loss: 1.7807\n",
      "Epoch: 6 / 30, Training Loss: 1.9513, Validation Loss: 1.7332\n",
      "Epoch: 7 / 30, Training Loss: 1.9167, Validation Loss: 1.6958\n",
      "Epoch: 8 / 30, Training Loss: 1.8877, Validation Loss: 1.6671\n",
      "Epoch: 9 / 30, Training Loss: 1.8612, Validation Loss: 1.6438\n",
      "Epoch: 10 / 30, Training Loss: 1.8361, Validation Loss: 1.6246\n",
      "Epoch: 11 / 30, Training Loss: 1.8125, Validation Loss: 1.6082\n",
      "Epoch: 12 / 30, Training Loss: 1.7912, Validation Loss: 1.5948\n",
      "Epoch: 13 / 30, Training Loss: 1.7711, Validation Loss: 1.5842\n",
      "Epoch: 14 / 30, Training Loss: 1.7516, Validation Loss: 1.5737\n",
      "Epoch: 15 / 30, Training Loss: 1.7336, Validation Loss: 1.5650\n",
      "Epoch: 16 / 30, Training Loss: 1.7169, Validation Loss: 1.5576\n",
      "Epoch: 17 / 30, Training Loss: 1.7011, Validation Loss: 1.5513\n",
      "Epoch: 18 / 30, Training Loss: 1.6860, Validation Loss: 1.5460\n",
      "Epoch: 19 / 30, Training Loss: 1.6717, Validation Loss: 1.5415\n",
      "Epoch: 20 / 30, Training Loss: 1.6581, Validation Loss: 1.5376\n",
      "Epoch: 21 / 30, Training Loss: 1.6452, Validation Loss: 1.5343\n",
      "Epoch: 22 / 30, Training Loss: 1.6329, Validation Loss: 1.5316\n",
      "Epoch: 23 / 30, Training Loss: 1.6211, Validation Loss: 1.5293\n",
      "Epoch: 24 / 30, Training Loss: 1.6098, Validation Loss: 1.5273\n",
      "Epoch: 25 / 30, Training Loss: 1.5990, Validation Loss: 1.5256\n",
      "Epoch: 26 / 30, Training Loss: 1.5886, Validation Loss: 1.5240\n",
      "Epoch: 27 / 30, Training Loss: 1.5786, Validation Loss: 1.5225\n",
      "Epoch: 28 / 30, Training Loss: 1.5689, Validation Loss: 1.5212\n",
      "Epoch: 29 / 30, Training Loss: 1.5596, Validation Loss: 1.5201\n",
      "Epoch: 30 / 30, Training Loss: 1.5506, Validation Loss: 1.5193\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "for ep in range(epochs):\n",
    "    tr_loss = train(st_model, st_optimizer, st_criterion, st_train_dl)\n",
    "    test_loss = test(st_model, st_val_dl, st_criterion)\n",
    "    print(f'Epoch: {ep+1} / {epochs}, Training Loss: {tr_loss[-1]:.4f}, Validation Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is training pretty nicely now. I'm seeing it max out around epoch 30 with a validation loss of 1.52. From there the training loss keeps coming down and the validation loss starts to climb again. This gives us some clues for what to do next to improve things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test this model out. How does it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello even\n",
      "which \"in which, owing of mistake of commof! I conscion by which\n",
      "so proved like the impating the persons, over him, and their fluthly mad be\n",
      "oblite, like the great decides something difficul synthety has also the most different injury of the more past-reventiment with otherses: a tritic in come amo's from bying to But of the freedom of being look the come\n",
      "uncertaities,\n",
      "science--thereby it excit\n"
     ]
    }
   ],
   "source": [
    "gen_text(st_model, \"Hello ev\", 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation looks pretty good! Structure and mainly real words. I'm not sure if I'm doing the generation completely efficiently / properly, because I'm passing in a new sequence each forward pass. Think about this some more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can I get better results with a GRU or LSTM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs, nl):\n",
    "        super().__init__()\n",
    "        self.vocab_size,self.nl = vocab_size,nl\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.LSTM(n_fac, n_hidden, nl, dropout=0.5)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h[0].size(1) != bs: self.init_hidden(bs)\n",
    "        outp,h = self.rnn(self.e(cs), self.h)\n",
    "#         print(type(h))\n",
    "        self.h = tuple(Variable(v.data) for v in h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        self.h = (Variable(torch.zeros(self.nl, bs, n_hidden)),\n",
    "                  Variable(torch.zeros(self.nl, bs, n_hidden)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden= 256\n",
    "n_layer = 2\n",
    "learning_rate=1e-3\n",
    "num_epochs=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLSTM = CharLSTM(vocab_size, emb_dim, batch_size, n_layer)\n",
    "optLSTM = torch.optim.Adam(modelLSTM.parameters(), lr=learning_rate)\n",
    "critLSTM = nn.CrossEntropyLoss();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(model, opt, crit, train_loader):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        opt.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        targets = targets.view(-1)\n",
    "        loss = crit(outputs, targets)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.data);\n",
    "        if i % 5 == 0:\n",
    "            print(f'Iteration: {i}, Training Loss: {loss.data}')\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(num_epochs):\n",
    "    tr_loss = train2(modelLSTM, optLSTM, critLSTM, st_train_dl)\n",
    "    test_loss = test(modelLSTM, st_val_dl, critLSTM)\n",
    "    print(f'Epoch: {ep+1} / {num_epochs}, Training Loss: {tr_loss[-1]:.4f}, Validation Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python insight",
   "language": "python",
   "name": "insight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
