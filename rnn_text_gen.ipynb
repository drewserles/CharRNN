{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the default tensor type at the top\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project goal is to create an RNN that operates at the character level to generate text. This means, given a sequence of characters, the RNN will generate the next character in the sequence. If the model is trained well, this can be done repeatedly to generate text that resembles the training corpus. I'll try to make everything as generic as possible so any source text can be used.  \n",
    "\n",
    "I've been working with the first two books from the Wheel of Time by Robert Jordan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4544942\n"
     ]
    }
   ],
   "source": [
    "# Import the text file as one big string\n",
    "path = \"../data/WoT/\"\n",
    "filenames = [\"WoT1.txt\", \"WoT2.txt\", \"WoT3.txt\"]\n",
    "text = \"\"\n",
    "for f in filenames:\n",
    "    text += open(f'{path}{f}').read()\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE WHEEL OF TIME\\n\\n\\n\\nBook Two\\n\\n\\n\\nTHE GREAT HUNT\\n\\n“Jordan has come to dominate the world that Tolkien began to reveal. . . . The battle scenes have the breathless urgency of firsthand experience, and the . . . evil laced into the forces of good, the dangers latent in any promised salvation, the sense of the unavoidable onslaug'"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[1711173:1711500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '©', '®', '–', '—', '‘', '’', '“', '”', '…', '™']\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The vocabulary is all the unique symbols used in the text. This is the real benefit of working with a character-\n",
    "level RNN. The vocabulary is tiny, and you don't need to deal with unkown words.\n",
    "\"\"\"\n",
    "chars = sorted(set(text))\n",
    "print(type(chars))\n",
    "print(chars)\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries from character->index and index->character\n",
    "c_to_idx = {c:i for i, c in enumerate(chars)}\n",
    "idx_to_c = {i:c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[85, 45, 60, 57, 1, 30, 77, 57, 1, 67]"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert whole text to indices. Want each character to be represented by its index in the vocabulary. This is how\n",
    "# we'll feed it to the RNN.\n",
    "text_idx = [c_to_idx[c] for c in text]\n",
    "text_len = len(text_idx)\n",
    "text_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best of its genre.”\n",
      "\n",
      "—The Ottawa Citizen\n",
      "\n",
      "\n",
      "\n",
      "“A splendid tale of heroic \n",
      "-----------\n",
      "the best of its genre.”\n",
      "\n",
      "—The Ottawa Citizen\n",
      "\n",
      "\n",
      "\n",
      "“A splendid tale of heroic \n"
     ]
    }
   ],
   "source": [
    "# Check it works to convert back: Join up the indices\n",
    "print(text[25:100])\n",
    "print(\"-----------\")\n",
    "print(''.join([idx_to_c[i] for i in text_idx[25:100]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence of characters passed to RNN at a time. This dictates the length of the unrolled model (# timesteps)\n",
    "# Batch size affects splitting of raw data as well as model architecture.\n",
    "seq_len = 8\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want a non-verlapping set of inputs and outputs. Each X should be equal to the sequence length. So should Y,\n",
    "# but should be shifted by 1. Want to shift X by the sequence length each step.\n",
    "# Note that we don't go right to the end so that there's room for Y and for a whole final sequence.\n",
    "idx_in_data = [text_idx[idx:idx+seq_len] for idx in range(0, text_len-1-seq_len, seq_len)]\n",
    "\n",
    "# idx_in_data = [[text_idx[idx] for idx in range(text_pt, text_pt+seq_len)] \\\n",
    "#                for text_pt in range(0, text_len - 1 - seq_len, seq_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75111, 8)\n",
      "[[39 41 28 29 24 26 28  0]\n",
      " [ 0  0 42 44 39 39 38 42]\n",
      " [32 37 30  1 72 60 53 72]]\n"
     ]
    }
   ],
   "source": [
    "# Convert these inputs into a numpy array and provide some info. Note that the dimensions are the total number of\n",
    "# sequences in the corpus and the sequence length\n",
    "inp = np.array(idx_in_data)\n",
    "print(inp.shape)\n",
    "print(inp[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same thing for Y\n",
    "idx_out_data = [text_idx[idx:idx+seq_len] for idx in range(1, text_len-seq_len, seq_len)]\n",
    "\n",
    "# idx_out_data = [[text_idx[idx] for idx in range(text_pt, text_pt+seq_len)] \\\n",
    "#                 for text_pt in range(1, text_len - seq_len, seq_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75111, 8)\n",
      "[[41 28 29 24 26 28  0  0]\n",
      " [ 0 42 44 39 39 38 42 32]\n",
      " [37 30  1 72 60 53 72  1]]\n"
     ]
    }
   ],
   "source": [
    "# Confirm that the target array is the input array shifted by 1. We'll be predicting the next character in the\n",
    "# sequence.\n",
    "outp = np.array(idx_out_data)\n",
    "print(outp.shape)\n",
    "print(outp[:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Split up the input and target data into training and test sets.\n",
    "Return 4 numpy arrays - training input, training targets, test input, and test targets\n",
    "'''\n",
    "def train_test_split(inp_data, out_data, train_fraction):\n",
    "    trn_idx = np.random.rand(len(inp_data)) < train_fraction\n",
    "    \n",
    "    inp_trn = inp_data[trn_idx]\n",
    "    inp_test = inp_data[~trn_idx]\n",
    "    \n",
    "    outp_trn = out_data[trn_idx]\n",
    "    outp_test = out_data[~trn_idx]\n",
    "    return inp_trn, outp_trn, inp_test, outp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 90% training, 10% test. This ratio could change with bigger corpus\n",
    "x_trn, y_trn, x_val, y_val = train_test_split(inp, outp, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PyTorch Dataset class for character level text generation.\n",
    "X and Y have widths equal to the sequence length.\n",
    "'''\n",
    "class CharSeqDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X);\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.X[idx];\n",
    "        label = self.Y[idx];\n",
    "        \n",
    "        return (item, label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation datasets\n",
    "train_ds = CharSeqDataset(x_trn, y_trn)\n",
    "val_ds = CharSeqDataset(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn these into PyTorch dataloaders with batch size = batch_size.\n",
    "# This will take care of the shuffling and batching,.\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True);\n",
    "val_dl = DataLoader(dataset=val_ds, batch_size=batch_size, shuffle=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple experiments with the data loaders.  \n",
    "1. The X and Y values are paired. Show that shuffling keeps the lined up.\n",
    "2. You get a different order whenever you iterate over a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_iter = iter(train_dl)\n",
    "x_exp, y_exp = next(exp_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[57, 56,  1, 72, 60, 57, 70, 57],\n",
      "        [77,  1, 72, 67,  1, 61, 65, 68]], device='cpu')\n",
      "*****\n",
      "torch.Size([512, 8])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[56,  1, 72, 60, 57, 70, 57, 54],\n",
      "        [ 1, 72, 67,  1, 61, 65, 68, 73]], device='cpu')\n"
     ]
    }
   ],
   "source": [
    "# Exp 1.\n",
    "print(x_exp.shape) # batch size by sequence length\n",
    "print(type(x_exp))\n",
    "print(x_exp[:2, :])\n",
    "print(\"*****\")\n",
    "print(y_exp.shape)\n",
    "print(type(y_exp))\n",
    "print(y_exp[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[54, 57, 71, 72,  7,  1, 75, 53],\n",
      "        [ 1, 72, 60, 53, 72,  1, 53, 70]], device='cpu')\n",
      "*****\n",
      "torch.Size([512, 8])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[57, 71, 72,  7,  1, 75, 53, 66],\n",
      "        [72, 60, 53, 72,  1, 53, 70, 57]], device='cpu')\n"
     ]
    }
   ],
   "source": [
    "# Exp 2.\n",
    "exp_iter2 = iter(train_dl)\n",
    "x_exp2, y_exp2 = next(exp_iter2)\n",
    "\n",
    "print(x_exp2.shape) # batch size by sequence length\n",
    "print(type(x_exp2))\n",
    "print(x_exp2[:2, :])\n",
    "print(\"*****\")\n",
    "print(y_exp2.shape)\n",
    "print(type(y_exp2))\n",
    "print(y_exp2[:2, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character level RNN model class, using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension for character's learned embeddings. Number of hidden units in the RNN\n",
    "emb_dim = 42\n",
    "n_hidden = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pytorch model.\n",
    "One sequence step involves embedding layer->RNN->fully connected layer->softmax over vocabulary\n",
    "A couple tricky points:\n",
    "-Want to keep the hidden activation values after a forward pass. So I have to detach h after a \n",
    "forward pass so BPTT doesn't have to go through all the steps back to the very beginning of the corpus.\n",
    "-Output predictions are rank 3 tensor of batch_size x seq_len x vocab length (it's a prediction over the vocab\n",
    "for each char in the sequence and for each sequence in the minibatch). Softmax only accepts rank 2, so need to\n",
    "reshape this into a (batch_size * seq_len) x vocab_length tensor.\n",
    "'''\n",
    "class CharRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, bs):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, emb_dim) # Going from vocab size down to embedding size\n",
    "        # Automatically runs for N sequence steps, which is known from input data size\n",
    "        self.rnn = nn.RNN(emb_dim, n_hidden) # embedding size to number of hidden units\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.h = self.init_h(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs.shape[0]\n",
    "        if self.h.shape[1] != bs:\n",
    "            self.h = self.init_h(bs)\n",
    "        inp = self.e(cs)\n",
    "        inp = torch.transpose(inp, 0, 1)\n",
    "        outp, h = self.rnn(inp, self.h)\n",
    "        self.h = Variable(h.data) # Save hidden values for next forward pass. Remove from BPTT by rewrapping in Var\n",
    "        outp = F.log_softmax(self.l_out(outp), dim=-1)\n",
    "        outp = torch.transpose(outp, 0, 1)\n",
    "        return outp.contiguous().view(-1, vocab_size) #This is tricky! Write myself a note it\n",
    "    \n",
    "    def init_h(self, bs):\n",
    "        return Variable(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function does 1 epoch (pass through the data)\n",
    "def train(model, opt, crit, train_loader):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        opt.zero_grad()\n",
    "        outputs = model(inputs.to(device))\n",
    "        targets = targets.view(-1).to(device)\n",
    "        loss = crit(outputs, targets)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.data);\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function calculates average loss over all the test data.\n",
    "def test(model, test_loader, crit):\n",
    "    # Put model in evaluation mode. Read up on what it does\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs.to(device))\n",
    "            targets = targets.view(-1).to(device)\n",
    "#             l = F.nll_loss(outputs, targets, reduction='sum').item() / len(targets)# sum up batch loss\n",
    "            l = crit(outputs, targets)\n",
    "            test_loss += l.item()\n",
    "            pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability (char index)\n",
    "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRnn(vocab_size, emb_dim, batch_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 4, Training Loss: 2.4561, Validation Loss: 2.4268\n",
      "Epoch: 2 / 4, Training Loss: 2.2769, Validation Loss: 2.2383\n",
      "Epoch: 3 / 4, Training Loss: 2.1676, Validation Loss: 2.1346\n",
      "Epoch: 4 / 4, Training Loss: 2.0905, Validation Loss: 2.0739\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "for ep in range(epochs):\n",
    "    tr_loss = train(model, optimizer, criterion, train_dl)\n",
    "    test_loss = test(model, val_dl, criterion)\n",
    "    print(f'Epoch: {ep+1} / {epochs}, Training Loss: {tr_loss[-1]:.4f}, Validation Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing - look at this quantitatively and see how well it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Given an input and a trained model, do a forward pass and predict the next character in the input sequence.\n",
    "Return this character as its integer index in the vocabulary.\n",
    "'''\n",
    "def next_letter(my_model, inp):\n",
    "\n",
    "    inp = torch.tensor([inp])\n",
    "    model_out = my_model(inp)\n",
    "    # Grab the last letter from the model output\n",
    "    # And sample from the vocabulary based on the weighted probability for character in the vocab.\n",
    "    # This makes this result non-deterministic, there can be variance between the next letter in the sequence\n",
    "    # depending on the sampling. Especially if multiple character get assigned similar probabilities.\n",
    "    next_idx = torch.multinomial(model_out[-1].exp(), 1).item()\n",
    "    \n",
    "    # return the next character index in the sequence\n",
    "    return next_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 ,\n"
     ]
    }
   ],
   "source": [
    "mytext = \"thos\"\n",
    "mytext = [c_to_idx[i] for i in mytext]\n",
    "nl = next_letter(model, mytext)\n",
    "print(nl, idx_to_c[nl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Keep generating the next character in the sequence. Repeatedly move the sampling window to include the latest\n",
    "prediction and predict the next letter. Goes for num_chars repetitions.\n",
    "'''\n",
    "def gen_text(my_model, inp, num_chars):\n",
    "    text = inp\n",
    "    inp = [c_to_idx[i] for i in inp]\n",
    "    for c in range(num_chars):\n",
    "        l = next_letter(my_model, inp)\n",
    "        text += idx_to_c[l]\n",
    "        inp = inp[1:]+[l]\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helloranoponof\n",
      "val angs by\n",
      "itsed Ipower alves feiks thems concaus hemean the itself cheasio revenoss, Qe NUE The dormed best of the againssicct is sas the aroutnt a ATSASNLTECOSSSothingle et mslever a saysantions out myencersind which ty that we all to the wor have nom the lstion of a dosed as what it. For smecopher cour, the we was, to the sersthoundikn Rir po once;--the loge houver\n",
      "man sen ghighatura\n"
     ]
    }
   ],
   "source": [
    "gen_text(model, \"Hello\", 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like it's starting to work alright, especially since it hasn't trained for too long. I found that benefits of continued training started to level off after ~30 epochs or so.\n",
    "\n",
    "I learned something important here though. When I split up the corpus into sequences of length 8 (sequence length / bptt length), characters 1 - 8 are the first training example in batch 1, 9 - 16 are the second etc.\n",
    "What that means is that the hidden states after the forward pass are meaningless for the next batch. There's no information gained about the previous sequence to help you out with the current sequence! \n",
    "\n",
    "Here's a different idea -> What if characters 1 - 8 make up the first training example of the first batch, then characters 9 - 16 make up the first training example of the second batch. That way (since we're saving activation values) when character 9 gets passed in as the first step to the RNN, the activations correspond to what came out after character 8, which was the last character of example 1 in the previous minibatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data up into \"vertical stacks\" as explained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 1711173\n",
      "Batch size: 512\n",
      "Sequence length / bptt length: 8\n"
     ]
    }
   ],
   "source": [
    "print(f'Corpus length: {len(text)}')\n",
    "print(f'Batch size: {batch_size}')\n",
    "print(f'Sequence length / bptt length: {seq_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to split the corpus into a number of chunks equal to the number of mini batches (512) because each chunk will represent a row example in successive minibatches. Also, the sequences need to still be seq_len long. So it's easiest to figure out how long (number of chars) a block can be if we need to get 512 into the corpus, then round that length to something evenly divisible by the sequene length. We lose a little bit of potential information, but it MAY be easier than having the final minibatch have a shorter sequence. Another option would be to zero-pad that last sequence, but I'm not going to worry about that since there's a lot of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-Pass in text, batch size, and sequence length to get back numpy array where consecutive text is lined up\n",
    "across minibatches.\n",
    "-Remember, in a list comprehension, the second for executes fully (it's the nested one). Each pass through the text, \n",
    "we grab a sequence lengthed bit of text from each \"mini-batch block\". Then next pass the index is shifted over.\n",
    "The idea is that you build an array where mini_batch example i makes continuous text across the mini batches,\n",
    "rather than within a minibatch.\n",
    "'''\n",
    "def vertical_chunk(text, bs, sl):\n",
    "    s_per_block = len(text) // sl // bs\n",
    "    c_per_block = s_per_block * sl\n",
    "    tl = c_per_block * bs\n",
    "    \n",
    "    r = [text[b+i : b+i+sl] for i in range(0,c_per_block,sl) for b in range(0,tl,c_per_block)]\n",
    "    return np.array(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_inp = vertical_chunk(text_idx, batch_size, seq_len)\n",
    "stacked_outp = vertical_chunk(text_idx[1:], batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(567808, 8)\n",
      "(567808, 8)\n"
     ]
    }
   ],
   "source": [
    "print(stacked_inp.shape)\n",
    "print(stacked_outp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85, 45, 60, 57, 1, 30, 77, 57, 1, 67, 58, 1, 72, 60, 57, 1]\n",
      "********\n",
      "[85 45 60 57  1 30 77 57]\n",
      "[45 60 57  1 30 77 57  1]\n",
      "********\n",
      "[ 1 67 58  1 72 60 57  1]\n",
      "[67 58  1 72 60 57  1 48]\n"
     ]
    }
   ],
   "source": [
    "# Show that continuous text is split over minibatch indices\n",
    "print(text_idx[:16])\n",
    "print(\"********\")\n",
    "print(stacked_inp[0])\n",
    "print(stacked_outp[0])\n",
    "print(\"********\")\n",
    "print(stacked_inp[512])\n",
    "print(stacked_outp[512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to make a dataloader. But we don't want to shuffle the data, because the continuity is important for the activations. So just split up the data into test and train by index, then make a dataloader without shuffle on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't want to randomly split. Just take the first half\n",
    "# st_x_trn, st_y_trn, st_x_val, st_y_val = train_test_split(stacked_inp, stacked_outp, 0.9)\n",
    "def data_split_nonrandom(in_data, out_data, train_frac):\n",
    "    portion = int(len(in_data) * train_frac)\n",
    "    return in_data[:portion], out_data[:portion], in_data[portion:], out_data[portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_x_trn, st_y_trn, st_x_val, st_y_val = data_split_nonrandom(stacked_inp, stacked_outp, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation datasets\n",
    "st_train_ds = CharSeqDataset(st_x_trn, st_y_trn)\n",
    "st_val_ds = CharSeqDataset(st_x_val, st_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unshuffled dataloaders\n",
    "# Is there a way to shuffle cross batch? Does this even have a point?\n",
    "st_train_dl = DataLoader(dataset=st_train_ds, batch_size=batch_size, shuffle=False);\n",
    "st_val_dl = DataLoader(dataset=st_val_ds, batch_size=batch_size, shuffle=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "tensor([85, 45, 60, 57,  1, 30, 77, 57], device='cpu')\n",
      "tensor([45, 60, 57,  1, 30, 77, 57,  1], device='cpu')\n"
     ]
    }
   ],
   "source": [
    "# Test that we're not shuffling\n",
    "test_iter = iter(st_train_dl)\n",
    "x_test, y_test = next(test_iter)\n",
    "print(x_test.shape)\n",
    "print(x_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "tensor([ 1, 67, 58,  1, 72, 60, 57,  1], device='cpu')\n",
      "tensor([67, 58,  1, 72, 60, 57,  1, 48], device='cpu')\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = next(test_iter)\n",
    "print(x_test.shape)\n",
    "print(x_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "tensor([48, 67, 70, 64, 56,  1, 61, 71], device='cpu')\n",
      "tensor([67, 70, 64, 56,  1, 61, 71,  1], device='cpu')\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = next(test_iter)\n",
    "print(x_test.shape)\n",
    "print(x_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48, 67, 70, 64, 56, 1, 61, 71]\n"
     ]
    }
   ],
   "source": [
    "print(text_idx[16:24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with vertically stacked data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data all looks great now. Try training the existing model and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_model = CharRnn(vocab_size, emb_dim, batch_size).to(device)\n",
    "st_optimizer = torch.optim.Adam(st_model.parameters(), lr=1e-3)\n",
    "st_criterion = nn.CrossEntropyLoss();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(st_train_dl) # Books 1 and 2 was 697"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 20, Training Loss: 2.1074, Validation Loss: 1.5616\n",
      "Epoch: 2 / 20, Training Loss: 1.9761, Validation Loss: 1.4362\n",
      "Epoch: 3 / 20, Training Loss: 1.8828, Validation Loss: 1.3812\n",
      "Epoch: 4 / 20, Training Loss: 1.7989, Validation Loss: 1.3507\n",
      "Epoch: 5 / 20, Training Loss: 1.7317, Validation Loss: 1.3318\n",
      "Epoch: 6 / 20, Training Loss: 1.6758, Validation Loss: 1.3188\n",
      "Epoch: 7 / 20, Training Loss: 1.6241, Validation Loss: 1.3085\n",
      "Epoch: 8 / 20, Training Loss: 1.5787, Validation Loss: 1.3001\n",
      "Epoch: 9 / 20, Training Loss: 1.5404, Validation Loss: 1.2934\n",
      "Epoch: 10 / 20, Training Loss: 1.5076, Validation Loss: 1.2879\n",
      "Epoch: 11 / 20, Training Loss: 1.4791, Validation Loss: 1.2835\n",
      "Epoch: 12 / 20, Training Loss: 1.4556, Validation Loss: 1.2798\n",
      "Epoch: 13 / 20, Training Loss: 1.4358, Validation Loss: 1.2768\n",
      "Epoch: 14 / 20, Training Loss: 1.4165, Validation Loss: 1.2744\n",
      "Epoch: 15 / 20, Training Loss: 1.3961, Validation Loss: 1.2725\n",
      "Epoch: 16 / 20, Training Loss: 1.3757, Validation Loss: 1.2708\n",
      "Epoch: 17 / 20, Training Loss: 1.3568, Validation Loss: 1.2694\n",
      "Epoch: 18 / 20, Training Loss: 1.3402, Validation Loss: 1.2681\n",
      "Epoch: 19 / 20, Training Loss: 1.3255, Validation Loss: 1.2669\n",
      "Epoch: 20 / 20, Training Loss: 1.3121, Validation Loss: 1.2657\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for ep in range(epochs):\n",
    "    tr_loss = train(st_model, st_optimizer, st_criterion, st_train_dl)\n",
    "    test_loss = test(st_model, st_val_dl, st_criterion)\n",
    "    print(f'Epoch: {ep+1} / {epochs}, Training Loss: {tr_loss[-1]:.4f}, Validation Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is training pretty nicely now. I'm seeing it max out around epoch 30 with a validation loss of 1.42 when looking at just the first book. From there the training loss keeps coming down and the validation loss starts to climb again. This gives us some clues for what to do next to improve things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test this model out. How does it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get those get going food? Didy despite he had be who, travel as thought climbed they would grace of passing.”\n",
      "\n",
      "Caution, if he sugs—but Perrin worse realized to be direction, the sniffed, some minute.”\n",
      "\n",
      "“I supposed,” Moiraine was any cource. Sandfilled; they’d be?”\n",
      "\n",
      "“So that had hardle of an edfew and never been much as he could does wool; she? I can’t calm taceles did not be.\n",
      "\n",
      "SLove for youthon, Loial, sh\n"
     ]
    }
   ],
   "source": [
    "gen_text(st_model, \"Get thos\", 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation looks pretty good! Structure and mainly real words. I'm not sure if I'm doing the generation completely efficiently / properly, because I'm passing in a new sequence each forward pass. Think about this some more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create all my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given some text, batch size, and sequence length, return the training and validation dataloaders\n",
    "def create_dataloader(text, bs, sl, spl_frac):\n",
    "    stacked_inp = vertical_chunk(text_idx, batch_size, sl)\n",
    "    stacked_outp = vertical_chunk(text_idx[1:], batch_size, sl)\n",
    "    st_x_trn, st_y_trn, st_x_val, st_y_val = data_split_nonrandom(stacked_inp, stacked_outp, spl_frac)\n",
    "    st_train_ds = CharSeqDataset(st_x_trn, st_y_trn)\n",
    "    st_val_ds = CharSeqDataset(st_x_val, st_y_val)\n",
    "    training_dl = DataLoader(dataset=st_train_ds, batch_size=batch_size, shuffle=False);\n",
    "    validation_dl = DataLoader(dataset=st_val_ds, batch_size=batch_size, shuffle=False);\n",
    "    return training_dl, validation_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "wot_sl = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try some data with different sequence lengths.\n",
    "wot_trn_dl, wot_val_dl = create_dataloader(text_idx, batch_size, wot_sl, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351 19\n"
     ]
    }
   ],
   "source": [
    "print(len(wot_trn_dl), len(wot_val_dl)) #length is dependent on sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 24])\n",
      "tensor([85, 45, 60, 57,  1, 30, 77, 57,  1, 67, 58,  1, 72, 60, 57,  1, 48, 67,\n",
      "        70, 64, 56,  1, 61, 71], device='cpu')\n",
      "tensor([45, 60, 57,  1, 30, 77, 57,  1, 67, 58,  1, 72, 60, 57,  1, 48, 67, 70,\n",
      "        64, 56,  1, 61, 71,  1], device='cpu')\n"
     ]
    }
   ],
   "source": [
    "# Test that everything looks ok. Sequence length is equal to wot_sl\n",
    "test_iter = iter(wot_trn_dl)\n",
    "x_test, y_test = next(test_iter)\n",
    "print(x_test.shape)\n",
    "print(x_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can I get better results with a GRU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpu(model, opt, crit, train_loader):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        opt.zero_grad()\n",
    "        outputs = model(inputs.cuda())\n",
    "        \n",
    "        targets = targets.view(-1).cuda()\n",
    "        loss = crit(outputs, targets)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.data);\n",
    "        if i+1 % 75 == 0:\n",
    "            print(f'Iteration: {i}, Training Loss: {loss.data}')\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function with gpu loading\n",
    "def test_gpu(model, test_loader, crit):\n",
    "    # Put model in evaluation mode. Read up on what it does\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs.cuda())\n",
    "            targets = targets.view(-1).cuda()\n",
    "#             l = F.nll_loss(outputs, targets, reduction='sum').item() / len(targets)# sum up batch loss\n",
    "            l = crit(outputs, targets)\n",
    "            test_loss += l.item()\n",
    "            pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability (char index)\n",
    "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, bs, num_layers):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.layers = num_layers\n",
    "        self.e = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, n_hidden, num_layers=self.layers, dropout=0.3)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs.shape[0]\n",
    "        if self.h.shape[1] != bs:\n",
    "            self.init_hidden(bs)\n",
    "        # Does the GRU note need a transpose and a transpose back?\n",
    "        inp = self.e(cs)\n",
    "        inp = torch.transpose(inp, 0, 1)\n",
    "        outp,h = self.rnn(inp, self.h)\n",
    "        self.h = Variable(h.data)\n",
    "        # Transpose back\n",
    "        outp = F.log_softmax(self.l_out(outp), dim=-1)\n",
    "        outp = torch.transpose(outp, 0, 1)\n",
    "        return outp.contiguous().view(-1, vocab_size) #This is tricky! Write myself a note it\n",
    "    \n",
    "    def init_hidden(self, bs): \n",
    "        self.h = Variable(torch.zeros(self.layers, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden= 256\n",
    "n_layer = 2\n",
    "learning_rate=1e-3\n",
    "num_epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelGRU = CharGRU(vocab_size, emb_dim, batch_size, n_layer).cuda()\n",
    "optGRU = torch.optim.Adam(modelGRU.parameters(), lr=learning_rate)\n",
    "critGRU = nn.CrossEntropyLoss();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 100, Training Loss: 1.7859, Validation Loss: 1.5686\n",
      "Epoch: 2 / 100, Training Loss: 1.6115, Validation Loss: 1.3610\n",
      "Epoch: 3 / 100, Training Loss: 1.5358, Validation Loss: 1.2873\n",
      "Epoch: 4 / 100, Training Loss: 1.4989, Validation Loss: 1.2493\n",
      "Epoch: 5 / 100, Training Loss: 1.4640, Validation Loss: 1.2242\n",
      "Epoch: 6 / 100, Training Loss: 1.4514, Validation Loss: 1.2077\n",
      "Epoch: 7 / 100, Training Loss: 1.4310, Validation Loss: 1.1954\n",
      "Epoch: 8 / 100, Training Loss: 1.4206, Validation Loss: 1.1854\n",
      "Epoch: 9 / 100, Training Loss: 1.4081, Validation Loss: 1.1768\n",
      "Epoch: 10 / 100, Training Loss: 1.3967, Validation Loss: 1.1707\n",
      "Epoch: 11 / 100, Training Loss: 1.3813, Validation Loss: 1.1644\n",
      "Epoch: 12 / 100, Training Loss: 1.3760, Validation Loss: 1.1592\n",
      "Epoch: 13 / 100, Training Loss: 1.3695, Validation Loss: 1.1554\n",
      "Epoch: 14 / 100, Training Loss: 1.3730, Validation Loss: 1.1513\n",
      "Epoch: 15 / 100, Training Loss: 1.3565, Validation Loss: 1.1484\n",
      "Epoch: 16 / 100, Training Loss: 1.3564, Validation Loss: 1.1455\n",
      "Epoch: 17 / 100, Training Loss: 1.3437, Validation Loss: 1.1427\n",
      "Epoch: 18 / 100, Training Loss: 1.3387, Validation Loss: 1.1410\n",
      "Epoch: 19 / 100, Training Loss: 1.3373, Validation Loss: 1.1392\n",
      "Epoch: 20 / 100, Training Loss: 1.3372, Validation Loss: 1.1372\n",
      "Epoch: 21 / 100, Training Loss: 1.3235, Validation Loss: 1.1351\n",
      "Epoch: 22 / 100, Training Loss: 1.3266, Validation Loss: 1.1353\n",
      "Epoch: 23 / 100, Training Loss: 1.3137, Validation Loss: 1.1321\n",
      "Epoch: 24 / 100, Training Loss: 1.3081, Validation Loss: 1.1310\n",
      "Epoch: 25 / 100, Training Loss: 1.3155, Validation Loss: 1.1294\n",
      "Epoch: 26 / 100, Training Loss: 1.3106, Validation Loss: 1.1294\n",
      "Epoch: 27 / 100, Training Loss: 1.2975, Validation Loss: 1.1279\n",
      "Epoch: 28 / 100, Training Loss: 1.2934, Validation Loss: 1.1280\n",
      "Epoch: 29 / 100, Training Loss: 1.2981, Validation Loss: 1.1260\n",
      "Epoch: 30 / 100, Training Loss: 1.3023, Validation Loss: 1.1260\n",
      "Epoch: 31 / 100, Training Loss: 1.2979, Validation Loss: 1.1254\n",
      "Epoch: 32 / 100, Training Loss: 1.2892, Validation Loss: 1.1247\n",
      "Epoch: 33 / 100, Training Loss: 1.2943, Validation Loss: 1.1240\n",
      "Epoch: 34 / 100, Training Loss: 1.2886, Validation Loss: 1.1235\n",
      "Epoch: 35 / 100, Training Loss: 1.2856, Validation Loss: 1.1222\n",
      "Epoch: 36 / 100, Training Loss: 1.2836, Validation Loss: 1.1233\n",
      "Epoch: 37 / 100, Training Loss: 1.2709, Validation Loss: 1.1222\n",
      "Epoch: 38 / 100, Training Loss: 1.2767, Validation Loss: 1.1216\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-486-e7de9b61f56b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelGRU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptGRU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritGRU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwot_trn_dl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#st_train_dl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelGRU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwot_val_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritGRU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {ep+1} / {num_epochs}, Training Loss: {tr_loss[-1]:.4f}, Validation Loss: {test_loss:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-481-7c0e9c57ef75>\u001b[0m in \u001b[0;36mtrain_gpu\u001b[0;34m(model, opt, crit, train_loader)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ep in range(num_epochs):\n",
    "    tr_loss = train_gpu(modelGRU, optGRU, critGRU, wot_trn_dl) #st_train_dl\n",
    "    test_loss = test_gpu(modelGRU, wot_val_dl, critGRU)\n",
    "    print(f'Epoch: {ep+1} / {num_epochs}, Training Loss: {tr_loss[-1]:.4f}, Validation Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rand isn’t liketask. Med masters halfwinterstones, we’lld the worlds Raal arresses and case from the Army look at him. Just notice and found. In abivemen. Few of the Seat Sundray old ever. As you are got and born ones and trying to detead to hide and you’re calling at all, but a little stripe I have someone of leaf as the Gaidin, a man right, and though well up—the Hurin when his tongues. Rand had to be s\n"
     ]
    }
   ],
   "source": [
    "gen_text(modelGRU, \"Rand isn\", 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation error is around 1.12 or so and the test output looks pretty decent. To get it better, I'll have to get creative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two layer LSTM. Possible to make it 1?\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs, nl):\n",
    "        super().__init__()\n",
    "        self.vocab_size,self.nl = vocab_size,nl\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.LSTM(n_fac, n_hidden, nl, dropout=0.5)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h[0].size(1) != bs: self.init_hidden(bs)\n",
    "        outp,h = self.rnn(self.e(cs), self.h)\n",
    "#         print(type(h))\n",
    "        self.h = tuple(Variable(v.data) for v in h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        self.h = (Variable(torch.zeros(self.nl, bs, n_hidden)),\n",
    "                  Variable(torch.zeros(self.nl, bs, n_hidden)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden= 256\n",
    "n_layer = 2\n",
    "learning_rate=1e-3\n",
    "num_epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLSTM = CharLSTM(vocab_size, emb_dim, batch_size, n_layer).cuda()\n",
    "optLSTM = torch.optim.Adam(modelLSTM.parameters(), lr=learning_rate)\n",
    "critLSTM = nn.CrossEntropyLoss();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 100, Training Loss: 2.6875, Validation Loss: 2.6872\n",
      "Epoch: 2 / 100, Training Loss: 2.5807, Validation Loss: 2.5795\n",
      "Epoch: 3 / 100, Training Loss: 2.5426, Validation Loss: 2.5384\n",
      "Epoch: 4 / 100, Training Loss: 2.5071, Validation Loss: 2.5186\n",
      "Epoch: 5 / 100, Training Loss: 2.5020, Validation Loss: 2.5079\n",
      "Epoch: 6 / 100, Training Loss: 2.4940, Validation Loss: 2.5011\n",
      "Epoch: 7 / 100, Training Loss: 2.4868, Validation Loss: 2.4962\n",
      "Epoch: 8 / 100, Training Loss: 2.4866, Validation Loss: 2.4924\n",
      "Epoch: 9 / 100, Training Loss: 2.4715, Validation Loss: 2.4898\n",
      "Epoch: 10 / 100, Training Loss: 2.4773, Validation Loss: 2.4878\n",
      "Epoch: 11 / 100, Training Loss: 2.4731, Validation Loss: 2.4863\n",
      "Epoch: 12 / 100, Training Loss: 2.4679, Validation Loss: 2.4852\n",
      "Epoch: 13 / 100, Training Loss: 2.4697, Validation Loss: 2.4840\n",
      "Epoch: 14 / 100, Training Loss: 2.4677, Validation Loss: 2.4833\n",
      "Epoch: 15 / 100, Training Loss: 2.4655, Validation Loss: 2.4827\n",
      "Epoch: 16 / 100, Training Loss: 2.4642, Validation Loss: 2.4818\n",
      "Epoch: 17 / 100, Training Loss: 2.4620, Validation Loss: 2.4814\n",
      "Epoch: 18 / 100, Training Loss: 2.4685, Validation Loss: 2.4811\n",
      "Epoch: 19 / 100, Training Loss: 2.4613, Validation Loss: 2.4808\n",
      "Epoch: 20 / 100, Training Loss: 2.4624, Validation Loss: 2.4805\n",
      "Epoch: 21 / 100, Training Loss: 2.4582, Validation Loss: 2.4802\n",
      "Epoch: 22 / 100, Training Loss: 2.4621, Validation Loss: 2.4799\n",
      "Epoch: 23 / 100, Training Loss: 2.4568, Validation Loss: 2.4799\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-9f46c9df483b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst_train_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst_val_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritLSTM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {ep+1} / {num_epochs}, Training Loss: {tr_loss[-1]:.4f}, Validation Loss: {test_loss:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-7c0e9c57ef75>\u001b[0m in \u001b[0;36mtrain_gpu\u001b[0;34m(model, opt, crit, train_loader)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ep in range(num_epochs):\n",
    "    tr_loss = train_gpu(modelLSTM, optLSTM, critLSTM, st_train_dl)\n",
    "    test_loss = test_gpu(modelLSTM, st_val_dl, critLSTM)\n",
    "    print(f'Epoch: {ep+1} / {num_epochs}, Training Loss: {tr_loss[-1]:.4f}, Validation Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
