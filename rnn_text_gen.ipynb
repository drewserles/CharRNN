{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project goal is to create an RNN that operates at the character level to generate text. This means, given a sequence of characters, the RNN will generate the next character in the sequence. If the model is trained well, this can be done repeatedly to generate text that resembles the training corpus. I'll try to make everything as generic as possible so any source text can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600893\n"
     ]
    }
   ],
   "source": [
    "# Import the text file as one big string\n",
    "path = \"../data/\"\n",
    "filename = \"nietzsche.txt\"\n",
    "text = open(f'{path}{filename}').read()\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Æ', 'ä', 'æ', 'é', 'ë']\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The vocabulary is all the unique symbols used in the text. This is the real benefit of working with a character-\n",
    "level RNN. The vocabulary is tiny, and you don't need to deal with unkown words.\n",
    "\"\"\"\n",
    "chars = sorted(set(text))\n",
    "print(type(chars))\n",
    "print(chars)\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries from character->index and index->character\n",
    "c_to_idx = {c:i for i, c in enumerate(chars)}\n",
    "idx_to_c = {i:c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39, 41, 28, 29, 24, 26, 28, 0, 0, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert whole text to indices. Want each character to be represented by its index in the vocabulary. This is how\n",
    "# we'll feed it to the RNN.\n",
    "text_idx = [c_to_idx[c] for c in text]\n",
    "text_len = len(text_idx)\n",
    "text_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth is a woman--what then? Is there not ground\n",
      "for suspecting that all ph\n",
      "-----------\n",
      "Truth is a woman--what then? Is there not ground\n",
      "for suspecting that all ph\n"
     ]
    }
   ],
   "source": [
    "# Check it works to convert back: Join up the indices\n",
    "print(text[25:100])\n",
    "print(\"-----------\")\n",
    "print(''.join([idx_to_c[i] for i in text_idx[25:100]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence of characters passed to RNN at a time. This dictates the length of the unrolled model (# timesteps)\n",
    "# Batch size affects splitting of raw data as well as model architecture.\n",
    "seq_len = 8\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want a non-verlapping set of inputs and outputs. Each X should be equal to the sequence length. So should Y,\n",
    "# but should be shifted by 1. Want to shift X by the sequence length each step.\n",
    "# Note that we don't go right to the end so that there's room for Y and for a whole final sequence.\n",
    "idx_in_data = [text_idx[idx:idx+seq_len] for idx in range(0, text_len-1-seq_len, seq_len)]\n",
    "\n",
    "# idx_in_data = [[text_idx[idx] for idx in range(text_pt, text_pt+seq_len)] \\\n",
    "#                for text_pt in range(0, text_len - 1 - seq_len, seq_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75111, 8)\n",
      "[[39 41 28 29 24 26 28  0]\n",
      " [ 0  0 42 44 39 39 38 42]\n",
      " [32 37 30  1 72 60 53 72]]\n"
     ]
    }
   ],
   "source": [
    "# Convert these inputs into a numpy array and provide some info. Note that the dimensions are the total number of\n",
    "# sequences in the corpus and the sequence length\n",
    "inp = np.array(idx_in_data)\n",
    "print(inp.shape)\n",
    "print(inp[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same thing for Y\n",
    "idx_out_data = [text_idx[idx:idx+seq_len] for idx in range(1, text_len-seq_len, seq_len)]\n",
    "\n",
    "# idx_out_data = [[text_idx[idx] for idx in range(text_pt, text_pt+seq_len)] \\\n",
    "#                 for text_pt in range(1, text_len - seq_len, seq_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75111, 8)\n",
      "[[41 28 29 24 26 28  0  0]\n",
      " [ 0 42 44 39 39 38 42 32]\n",
      " [37 30  1 72 60 53 72  1]]\n"
     ]
    }
   ],
   "source": [
    "# Confirm that the target array is the input array shifted by 1. We'll be predicting the next character in the\n",
    "# sequence.\n",
    "outp = np.array(idx_out_data)\n",
    "print(outp.shape)\n",
    "print(outp[:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Split up the input and target data into training and test sets.\n",
    "Return 4 numpy arrays - training input, training targets, test input, and test targets\n",
    "'''\n",
    "def train_test_split(inp_data, out_data, train_fraction):\n",
    "    trn_idx = np.random.rand(len(inp_data)) < train_fraction\n",
    "    \n",
    "    inp_trn = inp_data[trn_idx]\n",
    "    inp_test = inp_data[~trn_idx]\n",
    "    \n",
    "    outp_trn = out_data[trn_idx]\n",
    "    outp_test = out_data[~trn_idx]\n",
    "    return inp_trn, outp_trn, inp_test, outp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 90% training, 10% test. This ratio could change with bigger corpus\n",
    "x_trn, y_trn, x_val, y_val = train_test_split(inp, outp, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PyTorch Dataset class for character level text generation.\n",
    "X and Y have widths equal to the sequence length.\n",
    "'''\n",
    "class CharSeqDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X);\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.X[idx];\n",
    "        label = self.Y[idx];\n",
    "        \n",
    "        return (item, label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation datasets\n",
    "train_ds = CharSeqDataset(x_trn, y_trn)\n",
    "val_ds = CharSeqDataset(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn these into PyTorch dataloaders with batch size = batch_size.\n",
    "# This will take care of the shuffling and batching,.\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True);\n",
    "val_dl = DataLoader(dataset=val_ds, batch_size=batch_size, shuffle=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple experiments with the data loaders.  \n",
    "1. The X and Y values are paired. Show that shuffling keeps the lined up.\n",
    "2. You get a different order whenever you iterate over a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_iter = iter(train_dl)\n",
    "x_exp, y_exp = next(exp_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[ 1, 58, 67, 70, 59, 61, 74, 57],\n",
      "        [72, 60, 57, 65,  1, 72, 53, 71]])\n",
      "*****\n",
      "torch.Size([512, 8])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[58, 67, 70, 59, 61, 74, 57, 66],\n",
      "        [60, 57, 65,  1, 72, 53, 71, 72]])\n"
     ]
    }
   ],
   "source": [
    "# Exp 1.\n",
    "print(x_exp.shape) # batch size by sequence length\n",
    "print(type(x_exp))\n",
    "print(x_exp[:2, :])\n",
    "print(\"*****\")\n",
    "print(y_exp.shape)\n",
    "print(type(y_exp))\n",
    "print(y_exp[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[55, 67, 73, 66, 72, 57, 70,  1],\n",
      "        [55,  1, 65, 67, 56, 57,  1, 67]])\n",
      "*****\n",
      "torch.Size([512, 8])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[67, 73, 66, 72, 57, 70,  1, 68],\n",
      "        [ 1, 65, 67, 56, 57,  1, 67, 58]])\n"
     ]
    }
   ],
   "source": [
    "# Exp 2.\n",
    "exp_iter2 = iter(train_dl)\n",
    "x_exp2, y_exp2 = next(exp_iter2)\n",
    "\n",
    "print(x_exp2.shape) # batch size by sequence length\n",
    "print(type(x_exp2))\n",
    "print(x_exp2[:2, :])\n",
    "print(\"*****\")\n",
    "print(y_exp2.shape)\n",
    "print(type(y_exp2))\n",
    "print(y_exp2[:2, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character level RNN model class, using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension for character's learned embeddings. Number of hidden units in the RNN\n",
    "emb_dim = 42\n",
    "n_hidden = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pytorch model.\n",
    "One sequence step involves embedding layer->RNN->fully connected layer->softmax over vocabulary\n",
    "A couple tricky points:\n",
    "-Want to keep the hidden activation values after a forward pass. So I have to detach h after a \n",
    "forward pass so BPTT doesn't have to go through all the steps back to the very beginning of the corpus.\n",
    "-Output predictions are rank 3 tensor of batch_size x seq_len x vocab length (it's a prediction over the vocab\n",
    "for each char in the sequence and for each sequence in the minibatch). Softmax only accepts rank 2, so need to\n",
    "reshape this into a (batch_size * seq_len) x vocab_length tensor.\n",
    "'''\n",
    "class CharRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, bs):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, emb_dim) # Going from vocab size down to embedding size\n",
    "        # Automatically runs for N sequence steps, which is known from input data size\n",
    "        self.rnn = nn.RNN(emb_dim, n_hidden) # embedding size to number of hidden units\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.h = self.init_h(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs.shape[0]\n",
    "        if self.h.shape[1] != bs:\n",
    "            self.h = self.init_h(bs)\n",
    "        inp = self.e(cs)\n",
    "        inp = torch.transpose(inp, 0, 1)\n",
    "        outp, h = self.rnn(inp, self.h)\n",
    "        self.h = Variable(h.data) # Save hidden values for next forward pass. Remove from BPTT by rewrapping in Var\n",
    "        outp = F.log_softmax(self.l_out(outp), dim=-1)\n",
    "        outp = torch.transpose(outp, 0, 1)\n",
    "        return outp.contiguous().view(-1, vocab_size) #This is tricky! Write myself a note it\n",
    "    \n",
    "    def init_h(self, bs):\n",
    "        return Variable(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function does 1 epoch (pass through the data)\n",
    "def train(model, opt, crit, train_loader):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        opt.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        targets = targets.view(-1)\n",
    "        loss = crit(outputs, targets)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.data);\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function calculates average loss over all the test data.\n",
    "def test(model, test_loader, crit):\n",
    "    # Put model in evaluation mode. Read up on what it does\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            targets = targets.view(-1)\n",
    "#             l = F.nll_loss(outputs, targets, reduction='sum').item() / len(targets)# sum up batch loss\n",
    "            l = crit(outputs, targets)\n",
    "            test_loss += l.item()\n",
    "            pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability (char index)\n",
    "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRnn(vocab_size, emb_dim, batch_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 4, Training Loss: 2.4953, Validation Loss: 2.4161\n",
      "Epoch: 2 / 4, Training Loss: 2.2785, Validation Loss: 2.2416\n",
      "Epoch: 3 / 4, Training Loss: 2.2211, Validation Loss: 2.1428\n",
      "Epoch: 4 / 4, Training Loss: 2.0888, Validation Loss: 2.0796\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "for ep in range(epochs):\n",
    "    tr_loss = train(model, optimizer, criterion, train_dl)\n",
    "    test_loss = test(model, val_dl, criterion)\n",
    "    print(f'Epoch: {ep+1} / {epochs}, Training Loss: {tr_loss[-1]:.4f}, Validation Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing - look at this quantitatively and see how well it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Given an input and a trained model, do a forward pass and predict the next character in the input sequence.\n",
    "Return this character as its integer index in the vocabulary.\n",
    "'''\n",
    "def next_letter(my_model, inp):\n",
    "\n",
    "    inp = torch.tensor([inp])\n",
    "    model_out = my_model(inp)\n",
    "    # Grab the last letter from the model output\n",
    "    # And sample from the vocabulary based on the weighted probability for character in the vocab.\n",
    "    # This makes this result non-deterministic, there can be variance between the next letter in the sequence\n",
    "    # depending on the sampling. Especially if multiple character get assigned similar probabilities.\n",
    "    next_idx = torch.multinomial(model_out[-1].exp(), 1).item()\n",
    "    \n",
    "    # return the next character index in the sequence\n",
    "    return next_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 e\n"
     ]
    }
   ],
   "source": [
    "mytext = \"thos\"\n",
    "mytext = [c_to_idx[i] for i in mytext]\n",
    "nl = next_letter(model, mytext)\n",
    "print(nl, idx_to_c[nl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Keep generating the next character in the sequence. Repeatedly move the sampling window to include the latest\n",
    "prediction and predict the next letter. Goes for num_chars repetitions.\n",
    "'''\n",
    "def gen_text(my_model, inp, num_chars):\n",
    "    text = inp\n",
    "    inp = [c_to_idx[i] for i in inp]\n",
    "    print(inp)\n",
    "    for c in range(num_chars):\n",
    "        l = next_letter(my_model, inp)\n",
    "        text += idx_to_c[l]\n",
    "        inp = inp[1:]+[l]\n",
    "#         print(text)\n",
    "#         print(inp)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 57, 64, 64, 67]\n",
      "Hellog tethere formies. Heluty: in they the adny! Th!s that is some intrual--seneq. That e telve for precul compally of whace or\n",
      "mhan Has prch thightuch forturself soul that cas ats, causs, afre te stireds and small ragal\n",
      "  ior sopless\n",
      "dy, dimestions fage, even as, h phim lape\n",
      "lands of \" beveres. Bursen dv, tasting who a smelvedwer intthe\n",
      "spirelf our beave casity perf ooK theque'tive ps the, they\n",
      "\n",
      "107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen_text(model, \"Hello\", 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's working alright. Learned something important here though. When I split up the corpus into sequences of length 8 (sequence length / bptt length), characters 1 - 8 are the first training example in batch 1, 9 - 16 are the second etc.\n",
    "What that means is that the hidden states after the forward pass are meaningless for the next batch. There's no information gained about the previous sequence to help you out with the current sequence!\n",
    "Here's a different idea -> What if characters 1 - 8 make up the first training example of the first batch, then characters 9 - 16 make up the first training example of the second batch. That way (since we're saving activation values) when character 9 gets passed in as the first step to the RNN, the activations correspond to what came out after character 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge is how to split up the dataset now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 600893\n",
      "Batch size: 512\n",
      "Sequence length / bptt length: 8\n"
     ]
    }
   ],
   "source": [
    "print(f'Corpus length: {len(text)}')\n",
    "print(f'Batch size: {batch_size}')\n",
    "print(f'Sequence length / bptt length: {seq_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_cut = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75111, 8)\n",
      "(75111, 8)\n"
     ]
    }
   ],
   "source": [
    "#512*8 = 4096\n",
    "# print(len(text_idx)) = 600,893\n",
    "# inp and outp are the previous ones of shape 75111 x 8\n",
    "print(inp.shape)\n",
    "print(outp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1173"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_per_block = len(text_idx) // batch_size\n",
    "max_per_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146.702392578125"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_per_block / seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1168"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Throw away a tiny bit of information so each block is the same length\n",
    "block_len_rounded = max_per_block - (max_per_block % seq_len)\n",
    "block_len_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_block = [text_idx[i*block_len_rounded:i*block_len_rounded+block_len_rounded] for i in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "1168\n"
     ]
    }
   ],
   "source": [
    "print(len(vert_block))\n",
    "print(len(vert_block[511]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now need to re-piece this into an X by 8(seq_len) array where the first entry and the 512th entry are continuous\n",
    "# Note: the second loop is executed first? Yes it is.\n",
    "x = [n[i:i+seq_len] for i in range(0, len(vert_block[0]), seq_len) for n in vert_block ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74752\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(x))\n",
    "print(len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39, 41, 28, 29, 24, 26, 28, 0, 0, 0, 42, 44, 39, 39, 38, 42]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original layout\n",
    "vert_block[0][:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[67, 70, 65, 1, 67, 58, 1, 71, 73, 54, 62, 57, 55, 72, 8, 1]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vert_block[1][:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 41, 28, 29, 24, 26, 28, 0]\n",
      "[0, 0, 42, 44, 39, 39, 38, 42]\n",
      "**\n",
      "[67, 70, 65, 1, 67, 58, 1, 71]\n",
      "[73, 54, 62, 57, 55, 72, 8, 1]\n"
     ]
    }
   ],
   "source": [
    "# New layout. This should match above\n",
    "print(x[0])\n",
    "print(x[512])\n",
    "print(\"**\")\n",
    "print(x[1])\n",
    "print(x[513])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74752, 8)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_inp = np.array(x)\n",
    "chunk_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in text, batch size, and sequence length to get back numpy array where consecutive text is lined up\n",
    "# across minibatches.\n",
    "def vertical_chunk(text, bs, sl):\n",
    "    s_per_block = len(text) // sl // bs\n",
    "    c_per_block = s_per_block * sl\n",
    "    tl = c_per_block * bs\n",
    "    # groups of sequence length does each chunk get\n",
    "    r = [text[b+i:b+i+sl] for i in range(0,c_per_block,sl) for b in range(0,tl,c_per_block)]\n",
    "    return np.array(r)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_inp = vertical_chunk(text_idx, batch_size, seq_len)\n",
    "stacked_outp = vertical_chunk(text_idx[1:], batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74752, 8)\n",
      "(74752, 8)\n"
     ]
    }
   ],
   "source": [
    "print(stacked_inp.shape)\n",
    "print(stacked_outp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39 41 28 29 24 26 28  0]\n",
      "[41 28 29 24 26 28  0  0]\n",
      "********\n",
      "[ 0  0 42 44 39 39 38 42]\n",
      "[ 0 42 44 39 39 38 42 32]\n"
     ]
    }
   ],
   "source": [
    "# Nice, this is working now.\n",
    "print(stacked_inp[0])\n",
    "print(stacked_outp[0])\n",
    "print(\"********\")\n",
    "print(stacked_inp[512])\n",
    "print(stacked_outp[512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's working now. Do I want to make a dataloader? I guess so but without shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't want to randomly split. Just take the first half\n",
    "# st_x_trn, st_y_trn, st_x_val, st_y_val = train_test_split(stacked_inp, stacked_outp, 0.9)\n",
    "def data_split_nonrandom(in_data, out_data, train_frac):\n",
    "    portion = int(len(in_data) * train_frac)\n",
    "    return in_data[:portion], out_data[:portion], in_data[portion:], out_data[portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_x_trn, st_y_trn, st_x_val, st_y_val = data_split_nonrandom(stacked_inp, stacked_outp, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation datasets\n",
    "st_train_ds = CharSeqDataset(st_x_trn, st_y_trn)\n",
    "st_val_ds = CharSeqDataset(st_x_val, st_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39 41 28 29 24 26 28  0]\n",
      "[ 0  0 42 44 39 39 38 42]\n"
     ]
    }
   ],
   "source": [
    "print(stacked_inp[0])\n",
    "print(stacked_inp[512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unshuffled dataloaders\n",
    "# Is there a way to shuffle cross batch? Does this even have a point?\n",
    "st_train_dl = DataLoader(dataset=st_train_ds, batch_size=batch_size, shuffle=False);\n",
    "st_val_dl = DataLoader(dataset=st_val_ds, batch_size=batch_size, shuffle=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "tensor([39, 41, 28, 29, 24, 26, 28,  0])\n",
      "tensor([41, 28, 29, 24, 26, 28,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Test that we're not shuffling\n",
    "test_iter = iter(st_train_dl)\n",
    "x_test, y_test = next(test_iter)\n",
    "print(x_test.shape)\n",
    "print(x_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "tensor([ 0,  0, 42, 44, 39, 39, 38, 42])\n",
      "tensor([ 0, 42, 44, 39, 39, 38, 42, 32])\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = next(test_iter)\n",
    "print(x_test.shape)\n",
    "print(x_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "tensor([32, 37, 30,  1, 72, 60, 53, 72])\n",
      "tensor([37, 30,  1, 72, 60, 53, 72,  1])\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = next(test_iter)\n",
    "print(x_test.shape)\n",
    "print(x_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 37, 30, 1, 72, 60, 53, 72]\n"
     ]
    }
   ],
   "source": [
    "print(text_idx[16:24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data all looks great now. Try doing some training with this new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_model = CharRnn(vocab_size, emb_dim, batch_size)\n",
    "st_optimizer = torch.optim.Adam(st_model.parameters(), lr=1e-3)\n",
    "st_criterion = nn.CrossEntropyLoss();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 30, Training Loss: 2.4621, Validation Loss: 2.3361\n",
      "Epoch: 2 / 30, Training Loss: 2.2171, Validation Loss: 2.0658\n",
      "Epoch: 3 / 30, Training Loss: 2.1074, Validation Loss: 1.9289\n",
      "Epoch: 4 / 30, Training Loss: 2.0401, Validation Loss: 1.8369\n",
      "Epoch: 5 / 30, Training Loss: 1.9923, Validation Loss: 1.7731\n",
      "Epoch: 6 / 30, Training Loss: 1.9564, Validation Loss: 1.7274\n",
      "Epoch: 7 / 30, Training Loss: 1.9274, Validation Loss: 1.6926\n",
      "Epoch: 8 / 30, Training Loss: 1.9021, Validation Loss: 1.6654\n",
      "Epoch: 9 / 30, Training Loss: 1.8800, Validation Loss: 1.6446\n",
      "Epoch: 10 / 30, Training Loss: 1.8595, Validation Loss: 1.6286\n",
      "Epoch: 11 / 30, Training Loss: 1.8389, Validation Loss: 1.6137\n",
      "Epoch: 12 / 30, Training Loss: 1.8189, Validation Loss: 1.5994\n",
      "Epoch: 13 / 30, Training Loss: 1.8001, Validation Loss: 1.5878\n",
      "Epoch: 14 / 30, Training Loss: 1.7821, Validation Loss: 1.5781\n",
      "Epoch: 15 / 30, Training Loss: 1.7644, Validation Loss: 1.5699\n",
      "Epoch: 16 / 30, Training Loss: 1.7469, Validation Loss: 1.5629\n",
      "Epoch: 17 / 30, Training Loss: 1.7295, Validation Loss: 1.5568\n",
      "Epoch: 18 / 30, Training Loss: 1.7123, Validation Loss: 1.5514\n",
      "Epoch: 19 / 30, Training Loss: 1.6955, Validation Loss: 1.5466\n",
      "Epoch: 20 / 30, Training Loss: 1.6793, Validation Loss: 1.5423\n",
      "Epoch: 21 / 30, Training Loss: 1.6638, Validation Loss: 1.5387\n",
      "Epoch: 22 / 30, Training Loss: 1.6490, Validation Loss: 1.5357\n",
      "Epoch: 23 / 30, Training Loss: 1.6350, Validation Loss: 1.5332\n",
      "Epoch: 24 / 30, Training Loss: 1.6217, Validation Loss: 1.5312\n",
      "Epoch: 25 / 30, Training Loss: 1.6092, Validation Loss: 1.5295\n",
      "Epoch: 26 / 30, Training Loss: 1.5972, Validation Loss: 1.5281\n",
      "Epoch: 27 / 30, Training Loss: 1.5858, Validation Loss: 1.5268\n",
      "Epoch: 28 / 30, Training Loss: 1.5749, Validation Loss: 1.5257\n",
      "Epoch: 29 / 30, Training Loss: 1.5644, Validation Loss: 1.5246\n",
      "Epoch: 30 / 30, Training Loss: 1.5543, Validation Loss: 1.5237\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "for ep in range(epochs):\n",
    "    tr_loss = train2(st_model, st_optimizer, st_criterion, st_train_dl)\n",
    "    test_loss = test2(st_model, st_val_dl, st_criterion)\n",
    "    print(f'Epoch: {ep+1} / {epochs}, Training Loss: {tr_loss[-1]:.4f}, Validation Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Really training well now. Do some testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 57, 64, 64, 67, 1, 57, 74]\n",
      "Hello evenO: look the\n",
      "neighbously, from a develow regard on which if which\n",
      "no not \"pheilics untrily then have possible to\n",
      "play be belief to where commations (which belong: innotical misnops of which, and wishet freedest elevation, from striving in our science--almost been is not swate: they tastes FOR COMSENNEIPEST which\n",
      "at-like under that man, which this\n",
      "scienw one concernion freered, disevetify, as beli\n"
     ]
    }
   ],
   "source": [
    "gen_text(st_model, \"Hello ev\", 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can I get better results with an LSTM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden= 256\n",
    "n_layer = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs, nl):\n",
    "        super().__init__()\n",
    "        self.vocab_size,self.nl = vocab_size,nl\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.LSTM(n_fac, n_hidden, nl, dropout=0.5)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h[0].size(1) != bs: self.init_hidden(bs)\n",
    "        outp,h = self.rnn(self.e(cs), self.h)\n",
    "#         print(type(h))\n",
    "        self.h = tuple(Variable(v.data) for v in h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        self.h = (Variable(torch.zeros(self.nl, bs, n_hidden)),\n",
    "                  Variable(torch.zeros(self.nl, bs, n_hidden)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLSTM = CharSeqStatefulLSTM(vocab_size, emb_dim, batch_size, n_layer)\n",
    "optLSTM = torch.optim.Adam(modelLSTM.parameters(), lr=1e-3)\n",
    "critLSTM = nn.CrossEntropyLoss();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train3(model, opt, crit, train_loader):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        opt.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        targets = targets.view(-1)\n",
    "        loss = crit(outputs, targets)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.data);\n",
    "        if i % 5 == 0:\n",
    "            print(f'Iteration: {i}, Training Loss: {loss.data}')\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Training Loss: 4.424938678741455\n",
      "Iteration: 5, Training Loss: 4.038959980010986\n",
      "Iteration: 10, Training Loss: 3.1668179035186768\n",
      "Iteration: 15, Training Loss: 3.1785495281219482\n",
      "Iteration: 20, Training Loss: 3.131784439086914\n",
      "Iteration: 25, Training Loss: 3.1282219886779785\n",
      "Iteration: 30, Training Loss: 3.176347494125366\n",
      "Iteration: 35, Training Loss: 3.1276752948760986\n",
      "Iteration: 40, Training Loss: 3.102572202682495\n",
      "Iteration: 45, Training Loss: 3.0849645137786865\n",
      "Iteration: 50, Training Loss: 3.069145441055298\n",
      "Iteration: 55, Training Loss: 3.097275972366333\n",
      "Iteration: 60, Training Loss: 2.9951300621032715\n",
      "Iteration: 65, Training Loss: 2.9081475734710693\n",
      "Iteration: 70, Training Loss: 2.904214859008789\n",
      "Iteration: 75, Training Loss: 2.9010636806488037\n",
      "Iteration: 80, Training Loss: 2.8490655422210693\n",
      "Iteration: 85, Training Loss: 2.84846830368042\n",
      "Iteration: 90, Training Loss: 2.775449514389038\n",
      "Iteration: 95, Training Loss: 2.7863214015960693\n",
      "Iteration: 100, Training Loss: 2.823411703109741\n",
      "Iteration: 105, Training Loss: 2.7823593616485596\n",
      "Iteration: 110, Training Loss: 2.7251038551330566\n",
      "Iteration: 115, Training Loss: 2.7132790088653564\n",
      "Iteration: 120, Training Loss: 2.6937155723571777\n",
      "Iteration: 125, Training Loss: 2.753206253051758\n",
      "Iteration: 130, Training Loss: 2.7212562561035156\n",
      "Epoch: 1 / 4, Training Loss: 2.6870\n",
      "Iteration: 0, Training Loss: 2.741295576095581\n",
      "Iteration: 5, Training Loss: 2.700322151184082\n",
      "Iteration: 10, Training Loss: 2.679891586303711\n",
      "Iteration: 15, Training Loss: 2.6639795303344727\n",
      "Iteration: 20, Training Loss: 2.6862547397613525\n",
      "Iteration: 25, Training Loss: 2.7241053581237793\n",
      "Iteration: 30, Training Loss: 2.720478057861328\n",
      "Iteration: 35, Training Loss: 2.6599857807159424\n",
      "Iteration: 40, Training Loss: 2.7468645572662354\n",
      "Iteration: 45, Training Loss: 2.6847102642059326\n",
      "Iteration: 50, Training Loss: 2.6584503650665283\n",
      "Iteration: 55, Training Loss: 2.650547742843628\n",
      "Iteration: 60, Training Loss: 2.6251206398010254\n",
      "Iteration: 65, Training Loss: 2.6615822315216064\n",
      "Iteration: 70, Training Loss: 2.6166088581085205\n",
      "Iteration: 75, Training Loss: 2.627375841140747\n",
      "Iteration: 80, Training Loss: 2.660862445831299\n",
      "Iteration: 85, Training Loss: 2.6165361404418945\n",
      "Iteration: 90, Training Loss: 2.616065502166748\n",
      "Iteration: 95, Training Loss: 2.633236885070801\n",
      "Iteration: 100, Training Loss: 2.6021602153778076\n",
      "Iteration: 105, Training Loss: 2.5886049270629883\n",
      "Iteration: 110, Training Loss: 2.6229255199432373\n",
      "Iteration: 115, Training Loss: 2.6469948291778564\n",
      "Iteration: 120, Training Loss: 2.6087377071380615\n",
      "Iteration: 125, Training Loss: 2.5986244678497314\n",
      "Iteration: 130, Training Loss: 2.5710880756378174\n",
      "Epoch: 2 / 4, Training Loss: 2.5247\n",
      "Iteration: 0, Training Loss: 2.58766770362854\n",
      "Iteration: 5, Training Loss: 2.590859889984131\n",
      "Iteration: 10, Training Loss: 2.619281053543091\n",
      "Iteration: 15, Training Loss: 2.6467175483703613\n",
      "Iteration: 20, Training Loss: 2.5796635150909424\n",
      "Iteration: 25, Training Loss: 2.5601820945739746\n",
      "Iteration: 30, Training Loss: 2.572303295135498\n",
      "Iteration: 35, Training Loss: 2.6239054203033447\n",
      "Iteration: 40, Training Loss: 2.5769615173339844\n",
      "Iteration: 45, Training Loss: 2.6089677810668945\n",
      "Iteration: 50, Training Loss: 2.6049320697784424\n",
      "Iteration: 55, Training Loss: 2.543914318084717\n",
      "Iteration: 60, Training Loss: 2.5737881660461426\n",
      "Iteration: 65, Training Loss: 2.522139310836792\n",
      "Iteration: 70, Training Loss: 2.596181869506836\n",
      "Iteration: 75, Training Loss: 2.5304579734802246\n",
      "Iteration: 80, Training Loss: 2.5642290115356445\n",
      "Iteration: 85, Training Loss: 2.5811736583709717\n",
      "Iteration: 90, Training Loss: 2.5362014770507812\n",
      "Iteration: 95, Training Loss: 2.559835910797119\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-a37963e402ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#     test_loss = test2(modelLSTM, val_dl, critLSTM)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {ep+1} / {num_epochs}, Training Loss: {tr_loss[-1]:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, Validation Loss: {test_loss:.4f}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-fe88db8e43ec>\u001b[0m in \u001b[0;36mtrain3\u001b[0;34m(model, opt, crit, train_loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/insight/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/insight/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ep in range(num_epochs):\n",
    "    tr_loss = train3(modelLSTM, optLSTM, critLSTM, train_dl)\n",
    "#     test_loss = test2(modelLSTM, val_dl, critLSTM)\n",
    "    print(f'Epoch: {ep+1} / {num_epochs}, Training Loss: {tr_loss[-1]:.4f}')#, Validation Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python insight",
   "language": "python",
   "name": "insight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
